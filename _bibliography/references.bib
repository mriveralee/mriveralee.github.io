---
---
@article{optistructuresIMWUT2020,
author = {Swaminathan, Saiganesh and Fagert, Jonathon and Rivera, Michael and Cao, Andrew and Laput, Gierad and Noh, Hae Young and Hudson, Scott E.},
title = {OptiStructures: Fabrication of Room-Scale Interactive Structures with Embedded Fiber Bragg Grating Optical Sensors and Displays},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {2},
url = {https://doi.org/10.1145/3397310},
doi = {10.1145/3397310},
abstract = {A recent topic of considerable interest in the "smart building" community involves building interactive devices using sensors, and rapidly creating these objects using new fabrication methods. However, much of this work has been done at what might be called hand scale, with less attention paid to larger objects and structures (at furniture or room scales) despite the fact that we are very often literally surrounded by such objects. In this work, we present a new set of techniques for creating interactive objects at these scales. We demonstrate fabrication of both input sensors and displays directly into cast materials -those formed from a liquid or paste which solidifies in a mold; including, for example: concrete, plaster, polymer resins, and composites.Through our novel set of sensing and fabrication techniques, we enable human activity recognition at room scale and across a variety of materials. Our techniques create objects that appear the same as typical passive objects, but contain internal fiber optics for both input sensing and simple displays. We use a new fabrication device to inject optical fibers into CNC milled molds. Fiber Bragg Grating optical sensors configured as very sensitive vibration sensors are embedded in these objects. These require no internal power, can be placed at multiple locations along a single fiber, and can be interrogated from the end of the fiber. We evaluate the performance of our system by creating two full-scale application prototypes: an interactive wall, and an interactive table. With these prototypes, we demonstrate the ability of our system to sense a variety of human activities across eight different users. Our tests show that with suitable materials these sensors can detect and classify both direct interactions (such as tapping) and more subtle vibrations caused by activities such as walking across the floor nearby.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {50},
numpages = {21},
keywords = {optical fibers, interactive structures, embedded sensing}
}

@inproceedings{statelensUIST2019,
author = {Guo, Anhong and Kong, Junhan and Rivera, Michael and Xu, Frank F. and Bigham, Jeffrey P.},
title = {StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347873},
doi = {10.1145/3332165.3347873},
abstract = {Blind people frequently encounter inaccessible dynamic touchscreens in their everyday lives that are difficult, frustrating, and often impossible to use independently. Touchscreens are often the only way to control everything from coffee machines and payment terminals, to subway ticket machines and in-flight entertainment systems. Interacting with dynamic touchscreens is difficult non-visually because the visual user interfaces change, interactions often occur over multiple different screens, and it is easy to accidentally trigger interface actions while exploring the screen. To solve these problems, we introduce StateLens - a three-part reverse engineering solution that makes existing dynamic touchscreens accessible. First, StateLens reverse engineers the underlying state diagrams of existing interfaces using point-of-view videos found online or taken by users using a hybrid crowd-computer vision pipeline. Second, using the state diagrams, StateLens automatically generates conversational agents to guide blind users through specifying the tasks that the interface can perform, allowing the StateLens iOS application to provide interactive guidance and feedback so that blind users can access the interface. Finally, a set of 3D-printed accessories enable blind people to explore capacitive touchscreens without the risk of triggering accidental touches on the interface. Our technical evaluation shows that StateLens can accurately reconstruct interfaces from stationary, hand-held, and web videos; and, a user study of the complete system demonstrates that StateLens successfully enables blind users to access otherwise inaccessible dynamic touchscreens.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {371–385},
numpages = {15},
keywords = {dynamic interfaces, computer vision, touchscreen appliances, crowdsourcing, accessibility, reverse engineering, conversational agents},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{electrospinningCHI2019,
author = {Rivera, Michael L. and Hudson, Scott E.},
title = {Desktop Electrospinning: A Single Extruder 3D Printer for Producing Rigid Plastic and Electrospun Textiles},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300434},
doi = {10.1145/3290605.3300434},
abstract = {We present a new type of 3D printer that combines rigid plastic printing with melt electrospinning? a technique that uses electrostatic forces to create thin fibers from a molten polymer. Our printer enables custom-shaped textile sheets (similar in feel to wool felt) to be produced alongside rigid plastic using a single material (i.e., PLA) in a single process. We contribute open-source firmware, hardware specifications, and printing parameters to achieve melt electrospinning. Our approach offers new opportunities for fabricating interactive objects and sensors that blend the flexibility, absorbency and softness of produced electrospun textiles with the structure and rigidity of hard plastic for actuation, sensing, and tactile experiences.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {soft material fabrication, melt electrospinning, textiles, 3d printing},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}


@inproceedings{hydrogelTextilesCHI2020,
author = {Rivera, Michael L. and Forman, Jack and Hudson, Scott E. and Yao, Lining},
title = {Hydrogel-Textile Composites: Actuators for Shape-Changing Interfaces},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382788},
doi = {10.1145/3334480.3382788},
abstract = {The current work examines interactions that are enabled when depositing a human-safe hydrogel onto textile substrates. These hydrogel-textile composites are water-responsive, supporting reversible actuation. To enable these interactions, we describe a fabrication process using a consumer-grade 3D printer. We show how different combinations of printed hydrogel patterns and textiles create a rich actuator design space. Finally, we show an application of this approach and discuss opportunities for future work.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {shape-changing interfaces, hydrogels, digital fabrication, actuators, 3d printing, textiles, composites},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{inflatablesIMWUT2019,
author = {Swaminathan, Saiganesh and Rivera, Michael and Kang, Runchang and Luo, Zheng and Ozutemiz, Kadri Bugra and Hudson, Scott E.},
title = {Input, Output and Construction Methods for Custom Fabrication of Room-Scale Deployable Pneumatic Structures},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {2},
url = {https://doi.org/10.1145/3328933},
doi = {10.1145/3328933},
abstract = {In this paper, we examine the future of designing room-scale deployable pneumatic structures that can be fabricated with interactive capabilities and thus be responsive to human input and environments. While there have been recent advances in fabrication methods for creating large-scale structures, they have mainly focused around creating passive structures. Hence in this work, we collectively tackle three main challenges that need to be solved for designing room-scale interactive deployable structures namely -- the input, output (actuation) and construction methods. First, we explore three types of sensing methods --- acoustic, capacitive and pressure --- in order to embed input into these structures. These sensing methods enable users to perform gestures such as knock, squeeze and swipe with specific parts of our fabricated structure such as doors, windows, etc. and make them interactive. Second, we explore three types of actuation mechanisms -- inflatable tendon drive, twisted tendon drive and roll bending actuator -- that are implemented at structural scale and can be embedded into our structures to enable a variety of responsive actuation. Finally, we provide a construction method to custom fabricate and assemble inter-connected pneumatic trusses with embedded sensing and actuation capability to prototype interactions with room-scale deployable structures. To further illustrate the collective (input, output and construction) usage of the system, we fabricated three exemplar interactive deployable structures -- a responsive canopy, an interactive geodesic dome and a portable table (Figures 1 and 2). These can be deployed from a compact deflated state to a much larger inflated state which takes on a desired form while offering interactivity.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {62},
numpages = {17},
keywords = {embedded interactivity, digital fabrication, deployable pneumatic structures}
}

@inproceedings{maxifabDIS2018,
author = {McDonald, Joselyn and Zhao, Siyan and Liu, Jen and Rivera, Michael L.},
title = {MaxiFab: Applied Fabrication to Advance Period Technologies},
year = {2018},
isbn = {9781450356312},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197391.3205405},
doi = {10.1145/3197391.3205405},
abstract = {MaxiFab is a multifaceted collaborative effort aimed to address current shortcomings of menstrual technologies through digital fabrication techniques. We explore using 3D printing to produce customizable frames for sanitary napkins and laser cutting to fabricate fused washable sanitary napkins. Our preliminary explorations create menstrual products that address some of the most pressing problems with current period technologies---namely, access and cost barriers, waste, and lack of customization. Our work aims to reduce stigma regarding the discussion of menstruation while contextualizing the topic as an under-examined design research space.},
booktitle = {Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems},
pages = {13–19},
numpages = {7},
keywords = {digital fabrication, period technologies},
location = {Hong Kong, China},
series = {DIS '18 Companion}
}

@inproceedings{fabric3DPrinting2017,
author = {Rivera, Michael L. and Moukperian, Melissa and Ashbrook, Daniel and Mankoff, Jennifer and Hudson, Scott E.},
title = {Stretching the Bounds of 3D Printing with Embedded Textiles},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025460},
doi = {10.1145/3025453.3025460},
abstract = {Textiles are an old and well developed technology that have many desirable characteristics. They can be easily folded, twisted, deformed, or cut; some can be stretched; many are soft. Textiles can maintain their shape when placed under tension and can even be engineered with variable stretching ability. Conversely, 3D printing is a relatively new technology that can precisely produce functional, rigid objects with custom geometry. Combining 3D printing and textiles opens up new opportunities for rapidly creating rigid objects with embedded flexibility as well as soft materials imbued with additional functionality. In this paper, we introduce a suite of techniques for integrating 3D printing with textiles during the printing process, opening up a new design space that takes inspiration from both fields. We demonstrate how the malleability, stretchability and aesthetic qualities of textiles can enhance rigid printed objects, and how textiles can be augmented with functional properties enabled by 3D printing.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {497–508},
numpages = {12},
keywords = {3d printing, textiles, additive manufacturing, soft materials, interactive devices},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@article{trachealAire2016,
    author = {Gálvez, Jorge A. and Simpao, Allan F. and Dori, Yoav and Gralewski, Kevin and McGill, Nicholas H. and Rivera, Michael L. and Delso, Nile and Khan, Hammad and Rehman, Mohamed A. and Fiadjoe, John E.},
    title = {Not Just a Pretty Face: Three-Dimensional Printed Custom Airway Management Devices},
    journal = {3D Printing and Additive Manufacturing},
    volume = {3},
    number = {3},
    pages = {160-165},
    year = {2016},
    doi = {10.1089/3dp.2016.0025},

    URL = {
            https://doi.org/10.1089/3dp.2016.0025

    },
    eprint = {
            https://doi.org/10.1089/3dp.2016.0025

    }
}
